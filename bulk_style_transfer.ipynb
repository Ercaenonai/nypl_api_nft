{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500:, Style Loss : 0.000033, Content Loss : 11.681394\n",
      "Epoch 500:, Style Loss : 0.000032, Content Loss : 12.813873\n",
      "Epoch 500:, Style Loss : 0.000035, Content Loss : 12.801293\n",
      "Epoch 500:, Style Loss : 0.000033, Content Loss : 13.107387\n",
      "Epoch 500:, Style Loss : 0.000035, Content Loss : 13.259162\n",
      "Epoch 500:, Style Loss : 0.000034, Content Loss : 13.939225\n",
      "Epoch 500:, Style Loss : 0.000035, Content Loss : 13.143072\n",
      "Epoch 500:, Style Loss : 0.000036, Content Loss : 12.589658\n",
      "Epoch 500:, Style Loss : 0.000032, Content Loss : 12.489207\n",
      "Epoch 500:, Style Loss : 0.000034, Content Loss : 12.632652\n",
      "Epoch 500:, Style Loss : 0.000036, Content Loss : 12.947369\n",
      "Epoch 500:, Style Loss : 0.000036, Content Loss : 13.294169\n",
      "Epoch 500:, Style Loss : 0.000038, Content Loss : 12.990697\n",
      "Epoch 500:, Style Loss : 0.000032, Content Loss : 12.637298\n",
      "Epoch 500:, Style Loss : 0.000031, Content Loss : 12.983603\n",
      "Epoch 500:, Style Loss : 0.000033, Content Loss : 12.912018\n",
      "Epoch 500:, Style Loss : 0.000038, Content Loss : 12.722705\n",
      "Epoch 500:, Style Loss : 0.000036, Content Loss : 12.871089\n",
      "Epoch 500:, Style Loss : 0.000037, Content Loss : 12.860593\n",
      "Epoch 500:, Style Loss : 0.000035, Content Loss : 12.626818\n",
      "Epoch 500:, Style Loss : 0.000039, Content Loss : 12.656263\n",
      "Epoch 500:, Style Loss : 0.000030, Content Loss : 12.228970\n",
      "Epoch 500:, Style Loss : 0.000034, Content Loss : 13.010642\n",
      "Epoch 500:, Style Loss : 0.000034, Content Loss : 12.408864\n",
      "Epoch 500:, Style Loss : 0.000033, Content Loss : 12.162644\n",
      "Epoch 500:, Style Loss : 0.000031, Content Loss : 12.258641\n",
      "Epoch 500:, Style Loss : 0.000029, Content Loss : 12.319030\n",
      "Epoch 500:, Style Loss : 0.000031, Content Loss : 12.937190\n",
      "Epoch 500:, Style Loss : 0.000031, Content Loss : 12.274125\n",
      "Epoch 500:, Style Loss : 0.000033, Content Loss : 12.158538\n",
      "Epoch 500:, Style Loss : 0.000027, Content Loss : 12.873913\n",
      "Epoch 500:, Style Loss : 0.000034, Content Loss : 12.606960\n",
      "Epoch 500:, Style Loss : 0.000037, Content Loss : 12.548379\n",
      "Epoch 500:, Style Loss : 0.000035, Content Loss : 13.420707\n",
      "Epoch 500:, Style Loss : 0.000036, Content Loss : 13.342961\n",
      "Epoch 500:, Style Loss : 0.000035, Content Loss : 12.387319\n",
      "Epoch 500:, Style Loss : 0.000038, Content Loss : 13.052516\n",
      "Epoch 500:, Style Loss : 0.000037, Content Loss : 12.946752\n",
      "Epoch 500:, Style Loss : 0.000034, Content Loss : 12.997267\n",
      "Epoch 500:, Style Loss : 0.000037, Content Loss : 13.021807\n",
      "Epoch 500:, Style Loss : 0.000038, Content Loss : 12.978868\n",
      "Epoch 500:, Style Loss : 0.000036, Content Loss : 12.645262\n",
      "Epoch 500:, Style Loss : 0.000037, Content Loss : 12.590162\n",
      "Epoch 500:, Style Loss : 0.000033, Content Loss : 12.240483\n",
      "Epoch 500:, Style Loss : 0.000033, Content Loss : 12.592945\n",
      "Epoch 500:, Style Loss : 0.000038, Content Loss : 12.664339\n",
      "Epoch 500:, Style Loss : 0.000037, Content Loss : 12.669699\n",
      "Epoch 500:, Style Loss : 0.000036, Content Loss : 12.652018\n",
      "Epoch 500:, Style Loss : 0.000037, Content Loss : 12.697495\n",
      "Epoch 500:, Style Loss : 0.000038, Content Loss : 12.733863\n",
      "Epoch 500:, Style Loss : 0.000041, Content Loss : 12.543336\n",
      "Epoch 500:, Style Loss : 0.000040, Content Loss : 12.984269\n",
      "Epoch 500:, Style Loss : 0.000036, Content Loss : 12.437826\n",
      "Epoch 500:, Style Loss : 0.000036, Content Loss : 12.555057\n",
      "Epoch 500:, Style Loss : 0.000036, Content Loss : 12.672133\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision import transforms as tf\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "ROOT_PATH = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "IMAGE_FOLDER = 'folder with downloaded images'\n",
    "\n",
    "STYLE = 'style image folder'\n",
    "\n",
    "STYLE_IMAGE = 'image file'\n",
    "\n",
    "OUT_FOLDER = 'name of desired output folder'\n",
    "\n",
    "if not os.path.exists(os.path.join(ROOT_PATH, OUT_FOLDER)):\n",
    "    os.mkdir(os.path.join(ROOT_PATH, OUT_FOLDER))\n",
    "\n",
    "style_img = Image.open(os.path.join(ROOT_PATH, STYLE, STYLE_IMAGE)).convert('RGB')\n",
    "\n",
    "vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "vgg.to(device)\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "def transformation(img):\n",
    "    tasks = tf.Compose([tf.Resize(500),\n",
    "                        tf.ToTensor(),\n",
    "                        tf.Normalize(mean, std)])\n",
    "\n",
    "    img = tasks(img)\n",
    "\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "style_img = transformation(style_img).to(device)\n",
    "\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    image = tensor.clone().detach()\n",
    "\n",
    "    image = image.cpu().numpy().squeeze()\n",
    "\n",
    "    image = image.transpose(1, 2, 0)\n",
    "\n",
    "    image *= np.array(std) + np.array(mean)\n",
    "\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "LAYERS_OF_INTEREST = {'0': 'conv1_1',\n",
    "                      '5': 'conv2_1',\n",
    "                      '10': 'conv3_1',\n",
    "                      '19': 'conv4_1',\n",
    "                      '21': 'conv4_2',\n",
    "                      '28': 'conv5_1'}\n",
    "\n",
    "\n",
    "def apply_model_and_extract_features(image, model):\n",
    "    x = image\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "\n",
    "        if name in LAYERS_OF_INTEREST:\n",
    "            features[LAYERS_OF_INTEREST[name]] = x\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "style_img_features = apply_model_and_extract_features(style_img, vgg)\n",
    "\n",
    "\n",
    "def calculate_gram_matrix(tensor):\n",
    "    _, channels, height, width = tensor.size()\n",
    "\n",
    "    tensor = tensor.view(channels, height * width)\n",
    "\n",
    "    gram_matrix = torch.mm(tensor, tensor.t())\n",
    "\n",
    "    gram_matrix = gram_matrix.div(channels * height * width)\n",
    "\n",
    "    return gram_matrix\n",
    "\n",
    "\n",
    "style_features_gram_matrix = {layer: calculate_gram_matrix(style_img_features[layer]) for layer in style_img_features}\n",
    "\n",
    "weights = {'conv1_1': 1.0, 'conv2_1': 0.75, 'conv3_1': 0.35,\n",
    "           'conv4_1': 0.25, 'conv5_1': 0.15}\n",
    "\n",
    "img_lst = []\n",
    "\n",
    "# reads image filename from directory and appends to list\n",
    "for name in glob.glob(os.path.join(ROOT_PATH, IMAGE_FOLDER) + \"\\*.jpg\"):\n",
    "    img_lst.append(name)\n",
    "\n",
    "# sorts the list by numeric order to check output against input\n",
    "img_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "\n",
    "count = 0\n",
    "\n",
    "for image in img_lst:\n",
    "    content_img = Image.open(image).convert('RGB')\n",
    "\n",
    "    # enhancer = ImageEnhance.Brightness(content_img)\n",
    "    #\n",
    "    # factor = 1.3\n",
    "    #\n",
    "    # content_img = enhancer.enhance(factor)\n",
    "\n",
    "    content_img = transformation(content_img).to(device)\n",
    "\n",
    "    content_img_features = apply_model_and_extract_features(content_img, vgg)\n",
    "\n",
    "    target = content_img.clone().requires_grad_(True).to(device)\n",
    "\n",
    "    optimizer = optim.Adam([target], lr=0.003)\n",
    "\n",
    "    for i in range(1, 1000):\n",
    "        target_features = apply_model_and_extract_features(target, vgg)\n",
    "\n",
    "        content_loss = F.mse_loss(target_features['conv4_2'], content_img_features['conv4_2'])\n",
    "\n",
    "        style_loss = 0\n",
    "\n",
    "        for layer in weights:\n",
    "            target_feature = target_features[layer]\n",
    "\n",
    "            target_gram_matrix = calculate_gram_matrix(target_feature)\n",
    "\n",
    "            style_gram_matrix = style_features_gram_matrix[layer]\n",
    "\n",
    "            layer_loss = F.mse_loss(target_gram_matrix, style_gram_matrix)\n",
    "\n",
    "            layer_loss *= weights[layer]\n",
    "\n",
    "            _, channels, height, width = target_feature.shape\n",
    "\n",
    "            style_loss += layer_loss\n",
    "\n",
    "        total_loss = 1000000 * style_loss + content_loss\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print('Epoch {}:, Style Loss : {:4f}, Content Loss : {:4f}'.format(i, style_loss, content_loss))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    out = tensor_to_image(target)\n",
    "\n",
    "    img_out = Image.fromarray((out * 255).astype('uint8'), 'RGB')\n",
    "\n",
    "    img_out.save(os.path.join(ROOT_PATH, OUT_FOLDER) + f\"\\image_{str(count)}.png\",\n",
    "                 \"PNG\",\n",
    "                 progressive=True,\n",
    "                 quality=100,\n",
    "                 optimize=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

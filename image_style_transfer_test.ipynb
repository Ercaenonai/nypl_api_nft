{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "from PIL.ImageFilter import (\n",
    "   BLUR, CONTOUR, DETAIL, EDGE_ENHANCE, EDGE_ENHANCE_MORE,\n",
    "   EMBOSS, FIND_EDGES, SMOOTH, SMOOTH_MORE, SHARPEN\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision import transforms as tf\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "ROOT_PATH = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "IMAGE_FOLDER = 'content_images'\n",
    "\n",
    "STYLE = 'style_images'\n",
    "\n",
    "OUT_FOLDER = 'test'\n",
    "\n",
    "# will download the pre-trained pytorch model for style transfer\n",
    "vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "# set device to gpu if available. (CUDA required for fast processing)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "# assigns model to device\n",
    "vgg.to(device)\n",
    "\n",
    "# sets params for image normalization\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "# function for transforming images to arrays on tensor. the larger the value in tf.Resize, the longer it takes\n",
    "def transformation(img):\n",
    "\n",
    "    tasks = tf.Compose([tf.Resize(500),\n",
    "                        tf.ToTensor(),\n",
    "                        tf.Normalize(mean, std)])\n",
    "\n",
    "    img = tasks(img)\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "    return img\n",
    "\n",
    "# paths to content and style images\n",
    "content_img = Image.open(os.path.join(ROOT_PATH, IMAGE_FOLDER) + \"\\\\cacti_1.jpg\").convert('RGB')\n",
    "\n",
    "style_img = Image.open(os.path.join(ROOT_PATH, STYLE) + \"\\\\detailed_elephant.png\").convert('RGB')\n",
    "\n",
    "# optional image modifications see above PIL.ImageFilter imports for available options\n",
    "# content_img = content_img.filter(FIND_EDGES)\n",
    "#\n",
    "# content_img = content_img.filter(EMBOSS)\n",
    "\n",
    "# enhancer = ImageEnhance.Brightness(content_img)\n",
    "#\n",
    "# factor = 1.5\n",
    "# content_img = enhancer.enhance(factor)\n",
    "\n",
    "# assigns images to device\n",
    "content_img = transformation(content_img).to(device)\n",
    "\n",
    "style_img = transformation(style_img).to(device)\n",
    "\n",
    "\n",
    "# function for converting tensor arrays back to images\n",
    "def tensor_to_image(tensor):\n",
    "\n",
    "    image = tensor.clone().detach()\n",
    "    image = image.cpu().numpy().squeeze()\n",
    "\n",
    "    image = image.transpose(1, 2, 0)\n",
    "\n",
    "    image *= np.array(std) + np.array(mean)\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "# sets layers of network\n",
    "LAYERS_OF_INTEREST = {'0': 'conv1_1',\n",
    "                      '5': 'conv2_1',\n",
    "                      '10': 'conv3_1',\n",
    "                      '19': 'conv4_1',\n",
    "                      '21': 'conv4_2',\n",
    "                      '28': 'conv5_1'}\n",
    "\n",
    "\n",
    "# function for applying model and extracting features for content and style images\n",
    "def apply_model_and_extract_features(image, model):\n",
    "    x = image\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "\n",
    "        if name in LAYERS_OF_INTEREST:\n",
    "            features[LAYERS_OF_INTEREST[name]] = x\n",
    "\n",
    "    return features\n",
    "\n",
    "# sets extracted feature values\n",
    "content_img_features = apply_model_and_extract_features(content_img, vgg)\n",
    "style_img_features   = apply_model_and_extract_features(style_img, vgg)\n",
    "\n",
    "\n",
    "# function for calculating gram matrix\n",
    "def calculate_gram_matrix(tensor):\n",
    "\n",
    "    _, channels, height, width = tensor.size()\n",
    "\n",
    "    tensor = tensor.view(channels, height * width)\n",
    "\n",
    "    gram_matrix = torch.mm(tensor, tensor.t())\n",
    "\n",
    "    gram_matrix = gram_matrix.div(channels * height * width)\n",
    "\n",
    "    return gram_matrix\n",
    "\n",
    "\n",
    "style_features_gram_matrix = {layer: calculate_gram_matrix(style_img_features[layer]) for layer in style_img_features}\n",
    "\n",
    "# sets weights for each layer of interest\n",
    "weights = {'conv1_1': 1.0, 'conv2_1': 0.75, 'conv3_1': 0.35,\n",
    "           'conv4_1': 0.25, 'conv5_1': 0.15}\n",
    "\n",
    "target = content_img.clone().requires_grad_(True).to(device)\n",
    "\n",
    "# sets optimization and relu values\n",
    "optimizer = optim.Adam([target], lr=0.003)\n",
    "\n",
    "# applies network model in whatever range specified for number of iterations\n",
    "for i in range(1, 2000):\n",
    "\n",
    "    target_features = apply_model_and_extract_features(target, vgg)\n",
    "\n",
    "    content_loss = F.mse_loss (target_features['conv4_2'], content_img_features['conv4_2'])\n",
    "\n",
    "    style_loss = 0\n",
    "    for layer in weights:\n",
    "\n",
    "        target_feature = target_features[layer]\n",
    "\n",
    "        target_gram_matrix = calculate_gram_matrix(target_feature)\n",
    "        style_gram_matrix = style_features_gram_matrix[layer]\n",
    "\n",
    "        layer_loss = F.mse_loss (target_gram_matrix, style_gram_matrix)\n",
    "        layer_loss *= weights[layer]\n",
    "\n",
    "        _, channels, height, width = target_feature.shape\n",
    "\n",
    "        style_loss += layer_loss\n",
    "\n",
    "    total_loss = 1000000 * style_loss + content_loss\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print ('Epoch {}:, Style Loss : {:4f}, Content Loss : {:4f}'.format( i, style_loss, content_loss))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    total_loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "# displays original image and processed stylized image\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.imshow(tensor_to_image(content_img))\n",
    "ax2.imshow(tensor_to_image(target))\n",
    "\n",
    "# converts target image from tensor array back to image\n",
    "out = tensor_to_image(target)\n",
    "\n",
    "# converts output image to writeable format, resizes to original value\n",
    "img_out = Image.fromarray((out * 255).astype('uint8'), 'RGB')\n",
    "\n",
    "# displays final image to be saved\n",
    "img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use below to save test results\n",
    "\n",
    "# the below will create an output directory and save the image as png\n",
    "\n",
    "if not os.path.exists(os.path.join(ROOT_PATH, OUT_FOLDER)):\n",
    "    os.mkdir(os.path.join(ROOT_PATH, OUT_FOLDER))\n",
    "\n",
    "img_out.save(os.path.join(ROOT_PATH, OUT_FOLDER) + \"\\image_test\" + \".png\",\n",
    "                 \"PNG\",\n",
    "                 progressive=True,\n",
    "                 quality=100,\n",
    "                 optimize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}